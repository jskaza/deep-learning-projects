{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import brier_score_loss\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:19.035242Z","iopub.execute_input":"2023-03-16T06:13:19.036932Z","iopub.status.idle":"2023-03-16T06:13:19.044071Z","shell.execute_reply.started":"2023-03-16T06:13:19.036870Z","shell.execute_reply":"2023-03-16T06:13:19.042580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Datasets","metadata":{}},{"cell_type":"code","source":"def make_df(f: str, cols = None, gender = False) -> pd.DataFrame:\n    l = []\n    for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n        for filename in filenames:\n            path = Path(os.path.join(dirname, filename))\n            if path.stem[1:] == f:\n                if cols is not None:\n                    df = pd.read_csv(path, index_col = None, usecols = cols)\n                else:\n                    df = pd.read_csv(path, index_col = None)\n                if gender:\n                    df[\"Gender\"] = path.stem[0]\n                l.append(df)\n    return pd.concat(l, axis=0, ignore_index=True)\n\n\ndef double_merge(main: pd.DataFrame, supplementary: pd.DataFrame, how = \"left\") -> pd.DataFrame:\n    '''\n    used to merge two sets of team info per row (matchup)\n    '''\n    df = main.merge(supplementary, \n               left_on = [\"Season\",\"LowerTeamID\"],\n               right_on = [\"Season\",\"TeamID\"], how=how).merge(\n        supplementary, \n        left_on = [\"Season\",\"HigherTeamID\"],\n        right_on = [\"Season\",\"TeamID\"],\n        suffixes = [\"Lower\", \"Higher\"],\n        how=how\n    )\n    return df.drop(columns = [\"TeamIDHigher\", \"TeamIDLower\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:19.647894Z","iopub.execute_input":"2023-03-16T06:13:19.649570Z","iopub.status.idle":"2023-03-16T06:13:19.661266Z","shell.execute_reply.started":"2023-03-16T06:13:19.649505Z","shell.execute_reply":"2023-03-16T06:13:19.659822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teams = make_df(\"Teams\", [\"TeamID\"], True)\n\ntourney_seeds = make_df(\"NCAATourneySeeds\",[\"Season\",\"Seed\",\"TeamID\"])\ntourney_seeds[\"Seed\"] = tourney_seeds.Seed.apply(lambda x: int(x[1:-1]) if x[-1] in [\"a\", \"b\"] else int(x[1:]))\n\nconferences = make_df(\"TeamConferences\")\nconferences.ConfAbbrev = conferences.ConfAbbrev.replace({\n    \"pac_ten\": \"pac_twelve\",\n    \"mid_cont\": \"summit\"}\n)\nconferences.ConfAbbrev.value_counts()\nconferences[\"MidMajor\"] = (~conferences.ConfAbbrev.isin([\"sec\",\"big_ten\",\"acc\",\"pac_twelve\",\"aac\",\"big_east\",\"big_twelve\"])).astype(int)\nconferences.drop(columns=[\"ConfAbbrev\"], inplace=True)\n\n\nmassey = make_df(\"MasseyOrdinals_thru_Season2023_Day128\")\nmassey = massey[((massey.Season == 2023) & (massey.RankingDayNum == 128)) | ((massey.Season != 2023) & (massey.RankingDayNum == 133))]\nmassey = massey.groupby([\"Season\",\"TeamID\"])[\"OrdinalRank\"].median().reset_index(level=[\"Season\", \"TeamID\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:19.996863Z","iopub.execute_input":"2023-03-16T06:13:19.997425Z","iopub.status.idle":"2023-03-16T06:13:22.245054Z","shell.execute_reply.started":"2023-03-16T06:13:19.997368Z","shell.execute_reply":"2023-03-16T06:13:22.243362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compile regular season stats\nwinners = make_df(\"RegularSeasonDetailedResults\")\nwinners.rename(columns={\"WTeamID\": \"TeamID\", \"WScore\": \"Pts\", \"LTeamID\": \"OppID\", \"LScore\": \"PtsAllowed\"}, inplace=True)\nwinners.drop([\"WLoc\", \"DayNum\"], axis = 1, inplace = True)\nwinners.columns = [x[1:] if x.startswith(\"W\") else x for x in list(winners.columns)]\nwinners.columns = [f\"Opp{x[1:]}\" if x.startswith(\"L\") else x for x in list(winners.columns)]\nwinners[\"Win\"] = True\n\nlosers = winners.copy()\nlosers.rename(columns={\"TeamID\": \"OppID\", \"Pts\": \"PtsAllowed\", \"OppID\": \"TeamID\", \"PtsAllowed\": \"Pts\"}, inplace=True)\nlosers.columns = list(losers.columns)[:7] + list(losers.columns)[20:] + list(losers.columns)[7:20]\nlosers[\"Win\"] = False\n\nreg_season = pd.concat([winners, losers])\nreg_season.drop([\"OppID\"], axis = 1, inplace = True)\n\ngames_by_season = reg_season.groupby([\"Season\", \"TeamID\"]).count()\ntotals_by_season = reg_season.groupby([\"Season\", \"TeamID\"]).sum()\naverages_by_season = totals_by_season/games_by_season\naverages_by_season.reset_index(level=[\"Season\", \"TeamID\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:22.247491Z","iopub.execute_input":"2023-03-16T06:13:22.248041Z","iopub.status.idle":"2023-03-16T06:13:23.282367Z","shell.execute_reply.started":"2023-03-16T06:13:22.247984Z","shell.execute_reply":"2023-03-16T06:13:23.280726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tourney_results = make_df(\"NCAATourneyCompactResults\", [\"Season\", \"WTeamID\", \"LTeamID\"]) # 2023 won't be in training data implicitly\ndf = tourney_results.copy()\n\n# make the labels\ndf[\"LowerTeamID\"] = df[[\"WTeamID\",\"LTeamID\"]].min(axis=1)\ndf[\"HigherTeamID\"] = df[[\"WTeamID\",\"LTeamID\"]].max(axis=1)\ndf[\"Label\"] = (df.LowerTeamID == df.WTeamID).apply(int)\ndf.drop([\"WTeamID\",\"LTeamID\"], axis = 1, inplace = True)\n\n# add in X data\ndf = df.merge(teams, left_on = \"LowerTeamID\", right_on = \"TeamID\").drop(columns = \"TeamID\")\ndf = double_merge(df, tourney_seeds)\ndf = double_merge(df, conferences)\ndf = double_merge(df, averages_by_season, how=\"inner\") # only train on seasons with detailed data\ndf = double_merge(df, massey)\ndf[\"Female\"] = df.Gender == \"F\"\ndf.drop([\"LowerTeamID\",\"HigherTeamID\"], axis = 1, inplace = True)\ndf.drop(columns=[\"Gender\"], inplace=True)\n# women dont have massey, use mean amongst men tournament teams\n# since the datasets are so different, may be better to run separate analyses next time\nmeans = df[[\"OrdinalRankLower\", \"OrdinalRankHigher\"]].mean(axis=0)\ndf[[\"OrdinalRankLower\", \"OrdinalRankHigher\"]] = df[[\"OrdinalRankLower\", \"OrdinalRankHigher\"]].fillna(means)\n\nX = df.drop(columns = [\"Label\"])\ny = df.Label\n\nscaler = MinMaxScaler()\npreprocessor = scaler.fit(X)","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:23.285252Z","iopub.execute_input":"2023-03-16T06:13:23.286802Z","iopub.status.idle":"2023-03-16T06:13:23.425561Z","shell.execute_reply.started":"2023-03-16T06:13:23.286729Z","shell.execute_reply":"2023-03-16T06:13:23.423280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our validation is an assortment of random tournament games.\n","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=99)\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:23.428212Z","iopub.execute_input":"2023-03-16T06:13:23.428568Z","iopub.status.idle":"2023-03-16T06:13:23.464535Z","shell.execute_reply.started":"2023-03-16T06:13:23.428534Z","shell.execute_reply":"2023-03-16T06:13:23.463082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline: Coin Flip","metadata":{}},{"cell_type":"code","source":"brier_score_loss(y_val, np.repeat(0.5, len(y_val)))","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:24.427598Z","iopub.execute_input":"2023-03-16T06:13:24.429199Z","iopub.status.idle":"2023-03-16T06:13:24.440189Z","shell.execute_reply.started":"2023-03-16T06:13:24.429096Z","shell.execute_reply":"2023-03-16T06:13:24.438468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Method 1: Predict Using Historical Empirical Probabilities","metadata":{}},{"cell_type":"code","source":"historical_seeds = make_df(\"NCAATourneySeeds\",[\"Season\",\"Seed\",\"TeamID\"], True)\nhistorical_seeds[\"Female\"] = (historical_seeds.Gender == \"W\").astype(int)\nhistorical_seeds[\"Seed\"] = historical_seeds.Seed.apply(lambda x: int(x[1:-1]) if x[-1] in [\"a\", \"b\"] else int(x[1:]))\nhistorical_results = make_df(\"NCAATourneyCompactResults\", [\"Season\", \"WTeamID\", \"LTeamID\"])\nhistorical_results = historical_results.merge(historical_seeds, left_on = [\"Season\", \"WTeamID\"], right_on = [\"Season\", \"TeamID\"], how=\"left\")\nhistorical_results = historical_results.merge(historical_seeds, left_on = [\"Season\", \"LTeamID\"], right_on = [\"Season\", \"TeamID\"], how=\"left\", suffixes = [\"_Winner\", \"_Loser\"])[[\"Seed_Winner\",\"Seed_Loser\",\"Female_Winner\"]]\nhistorical_results[\"LowerSeed\"] = historical_results[[\"Seed_Winner\",\"Seed_Loser\"]].min(axis=1)\nhistorical_results[\"HigherSeed\"] = historical_results[[\"Seed_Winner\",\"Seed_Loser\"]].max(axis=1)\nhistorical_results[\"LowerSeedWins\"] = historical_results.Seed_Winner == historical_results[[\"Seed_Winner\",\"Seed_Loser\"]].min(axis=1)\nprobs = pd.DataFrame(historical_results.groupby([\"Female_Winner\",\"LowerSeed\",\"HigherSeed\"]).agg({\"LowerSeedWins\": [\"count\", np.mean]}).reset_index())\nprobs.columns = list(map(\"\".join, probs.columns.values))\nprobs[\"LowerSeedWinsmean\"] = np.where(probs.LowerSeed == probs.HigherSeed, 0.5, probs.LowerSeedWinsmean)\nfor g, d in probs.groupby(\"Female_Winner\"):\n    to_plot_1 = d[d.LowerSeed != d.HigherSeed].pivot_table(index=\"HigherSeed\", columns=\"LowerSeed\", values=\"LowerSeedWinsmean\")\n    to_plot_2 = d[d.LowerSeed != d.HigherSeed].pivot_table(index=\"HigherSeed\", columns=\"LowerSeed\", values=\"LowerSeedWinscount\")\n    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n    fig.suptitle(f\"Empirical Seed Matchup Probabilities and Counts (Women's = {g})\")\n    ax1 = sns.heatmap(to_plot_1, annot=True, cmap=\"PiYG\", ax=axs[0])\n    ax2 = sns.heatmap(to_plot_2, annot=True, cmap=\"PiYG\", ax=axs[1], fmt=\"g\")\n    ax1.title.set_text(\"Pr(Lower Seed Wins)\")\n    ax1.invert_yaxis()\n    ax2.title.set_text(\"Number of Matchups\")\n    ax2.invert_yaxis()","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:26.027488Z","iopub.execute_input":"2023-03-16T06:13:26.027987Z","iopub.status.idle":"2023-03-16T06:13:28.977642Z","shell.execute_reply.started":"2023-03-16T06:13:26.027946Z","shell.execute_reply":"2023-03-16T06:13:28.976487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prob_lookup(low, high, gender,team_predicting_is_lower_seed):\n    try:\n        prob = probs[(probs.Female_Winner == gender) & (probs.LowerSeed == low) & (probs.HigherSeed == high)][\"LowerSeedWinsmean\"].values[0]\n    except: # for never before seen seed matchups\n        prob = 0.5\n    if team_predicting_is_lower_seed == 1:\n        return prob\n    return 1-prob\n\ndef get_empirical_probs(arr: np.array):\n    lower_seeds = np.min(arr[:,:2], axis=1)\n    higher_seeds = np.max(arr[:,:2], axis=1)\n    team_predicting_is_lower_seed = np.argmax(arr[:,:2], axis=1)\n    genders = arr[:,2]\n    inputs = np.stack([lower_seeds, higher_seeds, genders, team_predicting_is_lower_seed], axis=1).tolist()\n    return list(map(lambda x: prob_lookup(x[0],x[1],x[2],x[3]), inputs))","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:28.979606Z","iopub.execute_input":"2023-03-16T06:13:28.980057Z","iopub.status.idle":"2023-03-16T06:13:28.992113Z","shell.execute_reply.started":"2023-03-16T06:13:28.980003Z","shell.execute_reply":"2023-03-16T06:13:28.990243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get the brier score by using historical probabilities. Note, this may be a bit artificially since the empirical probabilities are calculated with the validation data included. This is done because I want to use the best probability estimates in the final model.","metadata":{}},{"cell_type":"code","source":"y_pred = get_empirical_probs(X_val[[\"SeedLower\",\"SeedHigher\",\"Female\"]].to_numpy())\nbrier_score_loss(y_val, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:29.756955Z","iopub.execute_input":"2023-03-16T06:13:29.758525Z","iopub.status.idle":"2023-03-16T06:13:30.223818Z","shell.execute_reply.started":"2023-03-16T06:13:29.758455Z","shell.execute_reply":"2023-03-16T06:13:30.222119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Net\n\nThis dataset is tiny, meaning each training epoch is completed almost instantaneously. Thus, the strategy will be a low learning rate with a large number of epochs with an early stopping rule.","metadata":{}},{"cell_type":"code","source":"empirical_probs = np.expand_dims(get_empirical_probs(X_train[[\"SeedLower\",\"SeedHigher\",\"Female\"]].to_numpy()), 1)\nX_train = preprocessor.fit_transform(X_train)\nX_train = np.hstack([X_train, empirical_probs])\n\nempirical_probs = np.expand_dims(get_empirical_probs(X_val[[\"SeedLower\",\"SeedHigher\",\"Female\"]].to_numpy()), 1)\nX_val = preprocessor.fit_transform(X_val)\nX_val = np.hstack([X_val, empirical_probs])","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:32.687765Z","iopub.execute_input":"2023-03-16T06:13:32.688262Z","iopub.status.idle":"2023-03-16T06:13:34.558414Z","shell.execute_reply.started":"2023-03-16T06:13:32.688220Z","shell.execute_reply":"2023-03-16T06:13:34.557069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation=\"relu\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.000001)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"mse\"])\ncallback = tf.keras.callbacks.EarlyStopping(monitor=\"val_mse\", min_delta=0, patience=500,  restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:39.041422Z","iopub.execute_input":"2023-03-16T06:13:39.041825Z","iopub.status.idle":"2023-03-16T06:13:39.115129Z","shell.execute_reply.started":"2023-03-16T06:13:39.041788Z","shell.execute_reply":"2023-03-16T06:13:39.113603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=50000, batch_size=32, \n                    validation_data=(X_val, y_val),verbose = 0,\n                    callbacks = [callback])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-16T06:13:39.719737Z","iopub.execute_input":"2023-03-16T06:13:39.720255Z","iopub.status.idle":"2023-03-16T06:13:41.675091Z","shell.execute_reply.started":"2023-03-16T06:13:39.720212Z","shell.execute_reply":"2023-03-16T06:13:41.673801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logs = pd.DataFrame(history.history)\n\nplt.figure(figsize=(14, 4))\nplt.subplot(1, 2, 1)\nplt.plot(logs.loc[5:,\"loss\"], lw=2, label='Train')\nplt.plot(logs.loc[5:,\"val_loss\"], lw=2, label='Val')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(logs.loc[5:,\"mse\"], lw=2, label=\"Train\")\nplt.plot(logs.loc[5:,\"val_mse\"], lw=2, label=\"Val\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Brier Score\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:46.599328Z","iopub.execute_input":"2023-03-16T06:13:46.599846Z","iopub.status.idle":"2023-03-16T06:13:46.986665Z","shell.execute_reply.started":"2023-03-16T06:13:46.599801Z","shell.execute_reply":"2023-03-16T06:13:46.984444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(brier_score_loss(y_val, model.predict(X_val)))","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:50.539705Z","iopub.execute_input":"2023-03-16T06:13:50.540219Z","iopub.status.idle":"2023-03-16T06:13:50.739206Z","shell.execute_reply.started":"2023-03-16T06:13:50.540176Z","shell.execute_reply":"2023-03-16T06:13:50.737084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 5000, max_depth=5, random_state=99)\nrf.fit(X_train, y_train)\nbrier_score_loss(y_val, rf.predict_proba(X_val)[:,1])","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:53.313580Z","iopub.execute_input":"2023-03-16T06:13:53.314087Z","iopub.status.idle":"2023-03-16T06:14:22.503716Z","shell.execute_reply.started":"2023-03-16T06:13:53.314047Z","shell.execute_reply":"2023-03-16T06:14:22.502492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = list(X.columns) + [\"EmpiricalProb\"]\nimportances = rf.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(10,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:14:22.505913Z","iopub.execute_input":"2023-03-16T06:14:22.506367Z","iopub.status.idle":"2023-03-16T06:14:24.075784Z","shell.execute_reply.started":"2023-03-16T06:14:22.506334Z","shell.execute_reply":"2023-03-16T06:14:24.074405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\n\nEnsemble of Empirical Prediction, NN, and RF","metadata":{}},{"cell_type":"code","source":"matchups = pd.read_csv(\"/kaggle/input/march-machine-learning-mania-2023/SampleSubmission2023.csv\")\nmatchups[\"Season\"] = matchups.ID.apply(lambda x: int(x.split(\"_\")[0]))\nmatchups[\"LowerTeamID\"] = matchups.ID.apply(lambda x: int(x.split(\"_\")[1]))\nmatchups[\"HigherTeamID\"] = matchups.ID.apply(lambda x: int(x.split(\"_\")[2]))\nmatchups.drop(columns=[\"ID\",\"Pred\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:05:28.971279Z","iopub.status.idle":"2023-03-16T06:05:28.971987Z","shell.execute_reply.started":"2023-03-16T06:05:28.971642Z","shell.execute_reply":"2023-03-16T06:05:28.971679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = matchups.merge(teams, left_on = \"LowerTeamID\", right_on = \"TeamID\").drop(columns = \"TeamID\")\nX_test = double_merge(X_test, tourney_seeds)\nX_test = double_merge(X_test, conferences)\nX_test = double_merge(X_test, averages_by_season, how=\"inner\") # only train on seasons with detailed data\nX_test = double_merge(X_test, massey)\nX_test[\"Female\"] = X_test.Gender == \"F\"\nids = str(2023) + \"_\" + X_test.LowerTeamID.astype(str) + \"_\" + X_test.HigherTeamID.astype(str)\nX_test.drop([\"LowerTeamID\",\"HigherTeamID\"], axis = 1, inplace = True)\nX_test.drop(columns=[\"Gender\"], inplace=True)\n# # women dont have massey, use mean amongst men tournament teams\n# # since the datasets are so different, may be better to run separate analyses next time\nX_test[[\"OrdinalRankLower\", \"OrdinalRankHigher\"]] = X_test[[\"OrdinalRankLower\", \"OrdinalRankHigher\"]].fillna(means)\n# ensure columns ordered correctly\nX_test = X_test[X.columns]\nX_test[[\"SeedLower\",\"SeedHigher\"]] = X_test[[\"SeedLower\",\"SeedHigher\"]].fillna(8) # don't care about non-tourney teams\n\npred_empirical = np.expand_dims(get_empirical_probs(X_test[[\"SeedLower\",\"SeedHigher\",\"Female\"]].to_numpy()), 1)\n\nX_test = preprocessor.fit_transform(X_test)\nX_test = np.hstack([X_test, pred_empirical])\n\npred_nn = model.predict(X_test)\npred_rf = np.expand_dims(rf.predict_proba(X_test)[:,1], 1)\n\npreds = np.hstack((pred_empirical*0.1, pred_nn*0.8, pred_rf*0.1)).sum(axis=1)\n\nsubmission = pd.DataFrame({\"ID\":ids,\"Pred\":preds})\nsubmission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-03-16T06:13:08.785884Z","iopub.execute_input":"2023-03-16T06:13:08.786434Z","iopub.status.idle":"2023-03-16T06:13:09.760202Z","shell.execute_reply.started":"2023-03-16T06:13:08.786390Z","shell.execute_reply":"2023-03-16T06:13:09.758420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split back out the matchups for sanity checks\nsubmission[\"Season\"] = submission.ID.apply(lambda x: int(x.split(\"_\")[0]))\nsubmission[\"LowerTeamID\"] = submission.ID.apply(lambda x: int(x.split(\"_\")[1]))\nsubmission[\"HigherTeamID\"] = submission.ID.apply(lambda x: int(x.split(\"_\")[2]))\nsubmission = double_merge(submission, tourney_seeds).dropna()\nteam_info = make_df(\"Teams\", [\"TeamID\",\"TeamName\"], True)\nsubmission = submission.merge(team_info, left_on = \"LowerTeamID\", right_on = \"TeamID\")\nsubmission = submission.merge(team_info, left_on = \"HigherTeamID\", right_on = \"TeamID\", \n                              suffixes = [\"Lower\",\"Higher\"])\n# view some extreme examples\nexamples = pd.concat([submission.sort_values(\"Pred\", ascending=True).head(10),\n           submission.sort_values(\"Pred\", ascending=False).head(10)])\n\n(\"Prob \" + examples.TeamNameLower + \" beats \" + examples.TeamNameHigher + \" (\" + examples.GenderHigher +\n \"): \" + examples.Pred.apply(lambda x: str(round(x, 3)))).values\n   ","metadata":{"execution":{"iopub.status.busy":"2023-03-16T05:39:34.109264Z","iopub.execute_input":"2023-03-16T05:39:34.110272Z","iopub.status.idle":"2023-03-16T05:39:34.779774Z","shell.execute_reply.started":"2023-03-16T05:39:34.110209Z","shell.execute_reply":"2023-03-16T05:39:34.778144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}